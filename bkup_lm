#!/Users/rlozano/scripts/llms/simple/venv/bin/python3
import sys
import os
import importlib.util
import ollama
import logging

logging.getLogger("duckduckgo_search").setLevel(logging.WARNING)

MODEL_NAME = 'granite4:7b-a1b-h'

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SKILLS_DIR = os.path.join(BASE_DIR, "skills")
TOOLS_DIR = os.path.join(BASE_DIR, "tools")

def load_tools():
    """Loads schemas and functions from the tools/ directory."""
    tool_schemas = []
    tool_functions = {}
    if os.path.exists(TOOLS_DIR):
        for f in os.listdir(TOOLS_DIR):
            if f.endswith('.py') and f != '__init__.py':
                module_name = f[:-3]
                spec = importlib.util.spec_from_file_location(module_name, os.path.join(TOOLS_DIR, f))
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                if hasattr(module, 'SCHEMA'):
                    # Ensure the tool name in schema matches the filename
                    module.SCHEMA['function']['name'] = module_name
                    tool_schemas.append(module.SCHEMA)
                if hasattr(module, 'execute'):
                    tool_functions[module_name] = module.execute
    return tool_schemas, tool_functions

def main():
    piped_content = sys.stdin.read() if not sys.stdin.isatty() else ""
    manual_args = " ".join(sys.argv[1:])
    tool_schemas, tool_functions = load_tools()

    system_prompt = (
        "You are a local development assistant. "
        "IMPORTANT: You have tools to see the real filesystem. "
        "Do not guess filenames. Use 'list_dir' to see what is here."
    )

    messages = [
        {'role': 'system', 'content': system_prompt},
        {'role': 'user', 'content': manual_args + (f"\n\nCONTEXT:\n{piped_content}" if piped_content else "")}
    ]

    try:
        # THE AGENT LOOP: Allows the model to use tools and then respond
        while True:
            response = ollama.chat(model=MODEL_NAME, messages=messages, tools=tool_schemas)
            
            # Add the model's message (thought or tool call) to history
            messages.append(response.message)

            # If the model didn't call a tool, it's doneâ€”print result and exit
            if not response.message.tool_calls:
                print(response.message.content)
                break

            # If the model DID call tools, execute them
            for tool in response.message.tool_calls:
                func_name = tool.function.name
                # Ollama returns arguments as a dictionary
                args = tool.function.arguments 
                
                if func_name in tool_functions:
                    # Run the actual Python tool
                    result = tool_functions[func_name](**args)
                    # Feed the REAL data back into the conversation
                    messages.append({'role': 'tool', 'content': str(result)})
                else:
                    messages.append({'role': 'tool', 'content': f"Error: Tool {func_name} not found."})
            
            # The loop continues back to Ollama with the tool results

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)

if __name__ == "__main__":
    main()
